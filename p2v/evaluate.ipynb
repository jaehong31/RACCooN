{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import imageio\n",
    "import math\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "from typing import Dict, Optional, Sequence, List\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llava.model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llava.train.train import load_video, load_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=0\n",
    "import random\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_videos_grid(videos: torch.Tensor, path: str, rescale=False, n_rows=4, fps=8):\n",
    "    videos = rearrange(videos, \"b c t h w -> t b c h w\")\n",
    "    outputs = []\n",
    "    for x in videos:\n",
    "        x = torchvision.utils.make_grid(x, nrow=n_rows)\n",
    "        x = x.transpose(0, 1).transpose(1, 2).squeeze(-1)\n",
    "        if rescale:\n",
    "            x = (x + 1.0) / 2.0  # -1,1 -> 0,1\n",
    "        x = (x * 255).numpy().astype(np.uint8)\n",
    "        outputs.append(x)\n",
    "\n",
    "    # os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    imageio.mimsave(path, outputs, duration=1000 * (1 / fps), loop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_dtype = torch.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LlavaLlamaForCausalLM.from_pretrained('/nas-hdd/shoubin/pretrained_model/mgie_ckpt/LLaVA-Lightning-7B-delta-v1-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LLaVA.llava.model.video_diffusion.unet import VideoInpaintingModel\n",
    "model.unet = VideoInpaintingModel.from_pretrained('/nas-hdd/shoubin/pretrained_model/stable-diffusion-2-inpainting/', subfolder='unet_finetuned')\n",
    "model = model.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.text_encoder.to(weight_dtype)\n",
    "model.vae.to(weight_dtype)\n",
    "model.unet.to(weight_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "from einops import rearrange\n",
    "import random as rnd\n",
    "\n",
    "def load_mask(video_path, indices, mask_id, convert_to_box=False):\n",
    "    WIDTH = 512\n",
    "    HEIGHT = 320\n",
    "    \n",
    "    # print(video_path)\n",
    "    frame_files = list(sorted(os.listdir(video_path)))\n",
    "    frame_files = [x for x in frame_files if not x.startswith('.')]  # Excludes files like .DS_Store\n",
    "    selected_frames = [frame_files[i] for i in indices]\n",
    "    frames = []\n",
    "    \n",
    "    for frame_name in selected_frames:\n",
    "        image = Image.open(os.path.join(video_path, frame_name))\n",
    "        all_mask = np.array(image)\n",
    "        # mask = (all_mask == int(mask_id)).astype(np.uint8) * 255\n",
    "        mask = all_mask.astype(np.uint8) * 255\n",
    "        \n",
    "        if convert_to_box:\n",
    "            box_image = Image.new(\"L\", image.size, 255)\n",
    "            draw = ImageDraw.Draw(box_image)\n",
    "            # Find the bounding box of the mask\n",
    "            rows = np.any(mask, axis=1)\n",
    "            cols = np.any(mask, axis=0)\n",
    "            # box = (xmin, ymin, xmax, ymax)\n",
    "            if rows.any() and cols.any():  # Only proceed if there is at least one non-zero value\n",
    "                ymin, ymax = np.where(rows)[0][[0, -1]]\n",
    "                xmin, xmax = np.where(cols)[0][[0, -1]]\n",
    "                draw.rectangle([xmin , ymin, xmax, ymax], fill=0)\n",
    "        \n",
    "            box_image = box_image.resize((WIDTH, HEIGHT), resample=Image.BILINEAR)\n",
    "            box_np = np.array(box_image)\n",
    "            box_tensor = torch.from_numpy(box_np).float().div(255).unsqueeze(0)  # Add channel dimension\n",
    "            frames.append(box_tensor)\n",
    "        # Stack all tensors to create a batch\n",
    "            \n",
    "        else:\n",
    "            image = Image.fromarray(mask)\n",
    "            image = image.resize((WIDTH, HEIGHT), resample=Image.BILINEAR)\n",
    "            frames.append(image)\n",
    "    \n",
    "    if not convert_to_box:\n",
    "        # Stack images and convert to a tensor\n",
    "        frames = np.stack(frames, axis=2)\n",
    "        frames = torch.from_numpy(frames).permute(2, 0, 1).contiguous().unsqueeze(1)\n",
    "        frames = torch.where(frames > 0, torch.tensor(0.0), torch.tensor(1.0))\n",
    "    else:\n",
    "        frames = torch.stack(frames, dim=0)\n",
    "        frames = torch.where(frames > 0, torch.tensor(1.0), torch.tensor(0.0))\n",
    "    \n",
    "    return frames\n",
    "    \n",
    "def load_video(video_path, sample_num=16, sample_type='uniform', given_index=None):\n",
    "    WIDTH = 512\n",
    "    HEIGHT = 320\n",
    "    \n",
    "    \n",
    "    frame_files = list(sorted(os.listdir(video_path)))\n",
    "    # exclude .DS_Store\n",
    "    frame_files = [x for x in frame_files if x[0]!='.']\n",
    "    # print(frame_files)\n",
    "    vlen = len(frame_files)\n",
    "\n",
    "    n_frms = min(sample_num, vlen)\n",
    "    start, end = 0, vlen\n",
    "\n",
    "    if given_index is None:\n",
    "        intervals = np.linspace(start=start, stop=end, num=n_frms + 1).astype(int)\n",
    "        ranges = []\n",
    "        for idx, interv in enumerate(intervals[:-1]):\n",
    "            ranges.append((interv, intervals[idx + 1]))\n",
    "    \n",
    "        if sample_type == 'random':\n",
    "            indices = []\n",
    "            for x in ranges:\n",
    "                if x[0] == x[1]:\n",
    "                    indices.append(x[0])\n",
    "                else:\n",
    "                    indices.append(rnd.choice(range(x[0], x[1])))\n",
    "        elif sample_type == 'uniform':\n",
    "            indices = [(x[0] + x[1]) // 2 for x in ranges]\n",
    "        \n",
    "        selected_frames = [frame_files[i] for i in indices]\n",
    "        if len(selected_frames) < sample_num:\n",
    "            selected_frames += [frame_files[-1]] * (sample_num - len(selected_frames))\n",
    "            indices += [indices[-1]] * (sample_num - len(indices))\n",
    "    else:\n",
    "        selected_frames = [frame_files[i] for i in given_index]\n",
    "        indices = given_index\n",
    "    \n",
    "    # [:max_num_frames]\n",
    "    frames = []\n",
    "    # print(len(selected_frames))\n",
    "    for frame_name in selected_frames:\n",
    "        image = Image.open(os.path.join(video_path, frame_name)).convert(\"RGB\")\n",
    "        image = image.resize((WIDTH, HEIGHT), resample=Image.BILINEAR)\n",
    "        frames.append(image)\n",
    "\n",
    "    frames = np.stack(frames, axis=2)\n",
    "    frames = torch.from_numpy(frames).permute(2, 3, 0, 1).contiguous() #.unsqueeze(0)\n",
    "    frames = frames.float().div(255).clamp(0, 1).half() * 2.0 - 1.0\n",
    "    return frames, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvalDataset(Dataset):\n",
    "    def __init__(self,frame_num=16):\n",
    "        super(EvalDataset, self).__init__()\n",
    "        self.data = json.load(open('/nas-hdd/shoubin/videos/rovi/data/vplm_test.json'))\n",
    "        self.video_base_path = '/nas-hdd/shoubin/videos/rovi/data/JPEGImages/'\n",
    "        self.mask_base_path = '/nas-hdd/shoubin/videos/rovi/data/Annotations/'\n",
    "        self.inpainted_base_path = '/nas-hdd/shoubin/videos/rovi/data/InpaintImages/'\n",
    "        self.frame_num = frame_num\n",
    "        # self.tokenizer, self.multimodal_cfg = tokenizer, multimodal_cfgs\n",
    "        print('--num data: %d--'%(len(self.data)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "\n",
    "        anno = self.data[i]\n",
    "        vid = anno['vid']\n",
    "        task = anno['task']\n",
    "        mask_id = anno['mask_id']\n",
    "        \n",
    "        \n",
    "        if task == 'removal':\n",
    "            text = 'inpainted background' #anno['prompt'] #'inpainted background'\n",
    "            target, index = load_video(os.path.join(self.inpainted_base_path, vid, mask_id), sample_num=self.frame_num)\n",
    "            condition, _ = load_video(os.path.join(self.video_base_path, vid), sample_num=self.frame_num, given_index=index)\n",
    "            mask = load_mask(os.path.join(self.mask_base_path, vid), index, mask_id, convert_to_box=False)\n",
    "            \n",
    "        elif task == 'adding':\n",
    "            text = anno['description'] # anno['prompt']\n",
    "            target, index = load_video(os.path.join(self.video_base_path, vid), sample_num=self.frame_num)\n",
    "            condition, _ = load_video(os.path.join(self.inpainted_base_path, vid, mask_id), sample_num=self.frame_num, given_index=index)\n",
    "            mask = load_mask(os.path.join(self.mask_base_path, vid), index, mask_id, convert_to_box=True)\n",
    "        \n",
    "        elif task == 'editing':\n",
    "            text = anno['prompt']\n",
    "            target, index = load_video(os.path.join(self.video_base_path, vid), sample_num=self.frame_num)\n",
    "            condition, _ = load_video(os.path.join(self.video_base_path, vid), sample_num=self.frame_num, given_index=index)\n",
    "            mask = load_mask(os.path.join(self.mask_base_path, vid), index, mask_id, convert_to_box=False)\n",
    "            \n",
    "        data_dict = {}\n",
    "        data_dict['task'] = task\n",
    "        data_dict['target'] = target # [1, 8, 3, 320, 512]\n",
    "        data_dict['condition'] = condition # [1, 8, 3, 320, 512]\n",
    "        data_dict['mask'] = mask    # [1, 8, 3, 320, 512]\n",
    "        data_dict['text_prompt'] = text\n",
    "        return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = EvalDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(eval_dataset)):\n",
    "    input_dict = eval_dataset[i]\n",
    "    task = [input_dict['task']]\n",
    "    video = input_dict['condition'].to(weight_dtype).unsqueeze(0).to('cuda:0') # [16, 3, 320, 512]\n",
    "    mask = input_dict['mask'].to(weight_dtype).unsqueeze(0).to('cuda:0') # [16, 1, 320, 512]\n",
    "    text = [input_dict['text_prompt']]\n",
    "    # text = [short_dict[i]]\n",
    "    inpainted = model.inpaint(\n",
    "        video=video, # input video condition\n",
    "        mask=mask,\n",
    "        prompt=text,\n",
    "        task = task)\n",
    "    \n",
    "    if os.path.exists(\"result/{}/\".format(input_dict['task'])) == False:\n",
    "        os.makedirs(\"result/{}/\".format(input_dict['task']))\n",
    "    output_path = \"result/{}/{}.gif\".format(input_dict['task'], str(i))\n",
    "    save_videos_grid(inpainted, output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
